# ğŸ’¬ Prompts Vibe Coding - Aristote RAG Chatbot

Ce fichier contient les prompts Ã  utiliser avec **Claude Code** (CLI) entre chaque Ã©tape.

> **Mode d'emploi** : Lancez Claude Code depuis la racine du projet, puis copiez-collez les prompts.
> Les commits existants servent de **backup** si le code gÃ©nÃ©rÃ© a des problÃ¨mes.

---

## âš ï¸ PrÃ©requis Ollama (Ã  vÃ©rifier AVANT la dÃ©mo)

```bash
# 1. VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# 2. Installer le modÃ¨le d'embedding si pas fait
ollama pull nomic-embed-text

# 3. Tester un embedding
curl http://localhost:11434/api/embeddings -d '{
  "model": "nomic-embed-text",
  "prompt": "Test"
}'
```

---

## ğŸš€ Lancer Claude Code

```bash
cd aristote-rag-chatbot-demo-DRASI
claude
```

Ensuite, copiez-collez les prompts ci-dessous dans le terminal Claude Code.

---

## ğŸ“¦ Ã‰tape 1 â†’ 2 : Interface Streamlit

**Backup commit** : `189e823`

```
CrÃ©e une interface de chatbot avec Streamlit dans app.py.

L'interface doit avoir :
- Configuration de page avec titre "Aristote RAG Chatbot" et icÃ´ne ğŸ¤–
- Un titre principal et un sous-titre "DÃ©mo DRASI"
- L'historique des messages stockÃ© dans st.session_state.messages
- Affichage des messages avec st.chat_message
- Un champ de saisie avec st.chat_input
- Pour l'instant une rÃ©ponse placeholder "Connexion en cours..."
```

**Tester** : `streamlit run app.py`

---

## ğŸ–¥ï¸ Ã‰tape 2 â†’ 3 : Connexion Aristote

**Backup commit** : `393a206`

```
Ajoute la connexion Ã  l'API Aristote Dispatcher.

L'API est Ã  https://llm.ilaas.fr/v1 et est compatible OpenAI.

Dans la sidebar ajoute :
- Un st.text_input type password pour la clÃ© API (valeur par dÃ©faut depuis os.getenv)
- Une fonction get_client() avec @st.cache_resource qui crÃ©e le client OpenAI
- Une fonction get_available_models() avec @st.cache_data qui appelle client.models.list()
- Un selectbox pour choisir le modÃ¨le parmi ceux disponibles
- Un message de succÃ¨s quand la connexion fonctionne

Stocke le modÃ¨le sÃ©lectionnÃ© dans st.session_state.selected_model
```

**Tester** : Entrer le token API et voir les modÃ¨les apparaÃ®tre

---

## ğŸ”Œ Ã‰tape 3 â†’ 4 : Chat fonctionnel

**Backup commit** : `a9ad6b4`

```
Rends le chat fonctionnel.

Quand l'utilisateur envoie un message :
1. VÃ©rifie qu'un modÃ¨le est sÃ©lectionnÃ©
2. Affiche le message utilisateur
3. Ajoute-le Ã  l'historique
4. Appelle client.chat.completions.create avec :
   - Le modÃ¨le sÃ©lectionnÃ©
   - Un system prompt "Tu es un assistant helpful et rÃ©ponds en franÃ§ais."
   - Tout l'historique des messages
5. Affiche la rÃ©ponse avec un spinner pendant l'attente
6. Ajoute la rÃ©ponse Ã  l'historique

Ajoute un bouton "Effacer la conversation" dans la sidebar.
```

**Tester** : Poser quelques questions au chatbot

---

## ğŸ’¬ Ã‰tape 4 â†’ 5 : Upload documents

**Backup commit** : `bf9e480`

```
Ajoute l'upload de documents pour le RAG.

Dans la sidebar, crÃ©e une section "Base de connaissances" avec :
- st.header("ğŸ“š Base de connaissances")
- st.file_uploader pour PDF et DOCX avec accept_multiple_files=True
- Affichage du nombre de fichiers chargÃ©s
- Liste des noms de fichiers
```

---

## ğŸ“„ Ã‰tape 5 â†’ 6 : Extraction texte

**Backup commit** : `3bfbdf4`

```
Ajoute l'extraction de texte des documents.

CrÃ©e ces fonctions :
- extract_text_from_pdf(file_bytes) : utilise fitz (PyMuPDF) pour extraire le texte page par page
- extract_text_from_docx(file_bytes) : utilise python-docx et io.BytesIO
- extract_text(uploaded_file) : dispatch selon l'extension du fichier

Pour chaque fichier uploadÃ© :
- Extraire le texte seulement s'il n'est pas dÃ©jÃ  dans session_state.documents_text
- Afficher un aperÃ§u (300 caractÃ¨res) dans un st.expander
- Afficher le nombre de caractÃ¨res
```

**Tester** : Charger un PDF et voir le texte extrait

---

## ğŸ“ Ã‰tape 6 â†’ 7 : Chunking

**Backup commit** : `ba88d3a`

```
Ajoute le dÃ©coupage du texte en chunks.

CrÃ©e une fonction chunk_text(text, chunk_size=500, overlap=50) qui :
- DÃ©coupe le texte en morceaux de chunk_size caractÃ¨res maximum
- Ajoute un overlap entre les chunks pour la continuitÃ©
- Essaie de couper aux fins de phrases (". ", "? ", "! ", "\n")
- Retourne une liste de dicts avec : id, text, start, end

Modifie le traitement pour :
- CrÃ©er les chunks aprÃ¨s l'extraction
- Stocker text ET chunks dans session_state.documents_text[filename]
- Afficher le nombre de chunks dans l'expander

IMPORTANT - Ã‰viter boucle infinie :
Ã€ la fin de la boucle while, s'assurer que start progresse toujours :
  next_start = end - overlap
  if next_start <= start:
      start = start + 1  # Garantir progression
  else:
      start = next_start
```

**âš ï¸ Bug potentiel** : Sans la vÃ©rification ci-dessus, si `overlap >= taille rÃ©elle du chunk`, on peut avoir une boucle infinie causant un MemoryError.

---

## âœ‚ï¸ Ã‰tape 7 â†’ 8 : Embeddings avec Ollama ğŸ¦™

**Backup commit** : `0faee37` (version sentence-transformers)

```
Ajoute les embeddings avec Ollama.

J'ai Ollama installÃ© localement avec le modÃ¨le nomic-embed-text.
La librairie ollama est dÃ©jÃ  dans requirements.txt.

CrÃ©e ces fonctions :
- get_embedding(text) : 
  import ollama
  response = ollama.embeddings(model="nomic-embed-text", prompt=text)
  return response["embedding"]
  
- create_embeddings(chunks) : pour chaque chunk, gÃ©nÃ¨re l'embedding 
  avec get_embedding(chunk["text"]) et l'ajoute dans chunk["embedding"]
  Retourne les chunks enrichis

AprÃ¨s le chunking, appelle create_embeddings avec un spinner "CrÃ©ation des embeddings..."
```

**ğŸ”„ Alternative sentence-transformers** (si Ollama pose problÃ¨me) :

```
Ajoute les embeddings avec sentence-transformers Ã  la place.

Utilise le modÃ¨le "paraphrase-multilingual-MiniLM-L12-v2".

CrÃ©e :
- get_embedding_model() avec @st.cache_resource qui charge SentenceTransformer
- get_embedding(text) qui utilise model.encode()
- create_embeddings(chunks) qui encode tous les textes et ajoute les embeddings

Affiche un spinner pendant la crÃ©ation.
```

---

## ğŸ§® Ã‰tape 8 â†’ 9 : ChromaDB

**Backup commit** : `bd894e1`

```
Ajoute ChromaDB comme base vectorielle.

CrÃ©e :
- get_chroma_collection() avec @st.cache_resource :
  - Client en mÃ©moire avec Settings(anonymized_telemetry=False, allow_reset=True)
  - Collection "documents" avec metadata={"hnsw:space": "cosine"}

- add_to_vectorstore(chunks, filename) :
  - IDs : f"{filename}_{chunk['id']}"
  - Documents : le texte de chaque chunk
  - Embeddings : chunk["embedding"]
  - Metadatas : {"filename": filename, "chunk_id": chunk["id"]}

- search_similar(query, n_results=3) :
  - CrÃ©e l'embedding de la query avec get_embedding()
  - Appelle collection.query()
  - Retourne les chunks avec text, metadata, distance

AprÃ¨s create_embeddings, appelle add_to_vectorstore.
Affiche le total de chunks indexÃ©s dans la sidebar avec st.success.
```

---

## ğŸ—„ï¸ Ã‰tape 9 â†’ 10 : RAG complet

**Backup commit** : `d33aafe`

```
Connecte le RAG au chat.

Avant d'appeler Aristote :
1. Appelle search_similar(prompt, n_results=3)
2. Si des rÃ©sultats, formate le contexte :
   - Pour chaque chunk : "[Source: {filename}]\n{texte}"
   - SÃ©pare par "\n\n---\n\n"

3. Enrichis le system prompt avec le contexte :
   "Tu es un assistant helpful et rÃ©ponds en franÃ§ais.
   
   Tu as accÃ¨s aux documents suivants pour rÃ©pondre.
   Utilise ces informations et cite tes sources.
   Si l'info n'est pas dans les documents, dis-le clairement.
   
   === DOCUMENTS ===
   {contexte}
   === FIN DES DOCUMENTS ==="

4. Affiche les sources consultÃ©es dans un st.expander("ğŸ“š Sources consultÃ©es") avec :
   - Nom du fichier en gras
   - Score de similaritÃ© : 1 - distance (formatÃ© .2f)
   - AperÃ§u du texte (200 caractÃ¨res + "...")
```

**Tester** : Charger un document et poser une question sur son contenu

---

## ğŸ§  Ã‰tape 10 â†’ 11 : Polish final

**Backup commit** : `f55f9bd`

```
Ajoute les finitions UX.

Dans un st.expander("âš™ï¸ ParamÃ¨tres RAG") dans la sidebar :
- st.toggle "Activer le RAG" (dÃ©faut True)
- st.slider taille des chunks (200-1000, dÃ©faut 500, step 50)
- st.slider chevauchement (0-200, dÃ©faut 50, step 10)
- st.slider nombre de sources (1-10, dÃ©faut 3)
- Stocke tout dans st.session_state.rag_params

En haut de la page principale, aprÃ¨s le titre :
- Si RAG actif et collection.count() > 0 : st.info avec le nombre de chunks
- Si RAG actif et count == 0 : st.warning "Aucun document chargÃ©"
- Si RAG dÃ©sactivÃ© : st.caption "Mode conversation simple"

Dans la sidebar, aprÃ¨s l'affichage des documents :
- Bouton "ğŸ”„ RÃ©initialiser la base" qui :
  - Appelle client.reset() sur ChromaDB
  - Vide session_state.documents_text
  - Appelle st.cache_resource.clear()
  - Appelle st.rerun()

Modifie search_similar pour respecter rag_params["enabled"].
Mets Ã  jour le README avec les fonctionnalitÃ©s et prÃ©requis Ollama.
```

---

## ğŸ”’ Ã‰tape 11 â†’ 12 : Mode RAG exclusif (anti-hallucination)

**Backup commit** : `06e5a4f`

```
Ajoute un mode "RAG exclusif" pour Ã©viter les hallucinations.

Dans les paramÃ¨tres RAG, ajoute un toggle "ğŸ”’ Mode exclusif" :
- DÃ©sactivÃ© si RAG dÃ©sactivÃ© (disabled=not rag_enabled)
- Help : "Si activÃ©, le chatbot ne rÃ©pond QU'avec les documents"

Stocke rag_params["exclusive"] dans session_state.

Quand ce mode est actif, modifie le system prompt pour Ãªtre STRICT :
- "Tu ne dois utiliser QUE les informations des documents ci-dessous"
- "Tu ne dois JAMAIS inventer ou utiliser tes connaissances gÃ©nÃ©rales"
- "Si l'info n'est pas prÃ©sente, rÃ©ponds : 'Cette information n'est pas prÃ©sente dans les documents'"
- "Cite toujours la source (nom du document)"

Si mode exclusif mais aucun contexte trouvÃ© :
- Affiche st.warning avec message explicatif
- Ajoute un message Ã  l'historique disant qu'aucune info n'a Ã©tÃ© trouvÃ©e
- Ne PAS appeler le LLM

Mets Ã  jour l'indicateur en haut de page :
- Mode exclusif actif : st.warning "ğŸ”’ Mode RAG EXCLUSIF - X chunks (rÃ©ponses uniquement depuis les documents)"
```

**ğŸ’¡ IntÃ©rÃªt pÃ©dagogique** : 

Ce mode montre la diffÃ©rence entre :
- **RAG augmentÃ©** : le LLM utilise documents + connaissances gÃ©nÃ©rales
- **RAG exclusif** : le LLM utilise UNIQUEMENT les documents (zÃ©ro hallucination)

**DÃ©monstration suggÃ©rÃ©e** :
1. Charger un document sur un sujet prÃ©cis
2. Poser une question sur le contenu â†’ rÃ©ponse avec source
3. Poser une question hors sujet â†’ "Information non trouvÃ©e"
4. DÃ©sactiver le mode exclusif â†’ le LLM rÃ©pond avec ses connaissances

---

## ğŸ¦™ Ã‰tape 12 â†’ 14 : Migration vers Ollama (embeddings optimisÃ©s)

**Backup commit** : `2b6bf6f`

```
Migre les embeddings de sentence-transformers vers Ollama pour plus de performance.

PrÃ©requis : Ollama doit Ãªtre lancÃ© avec le modÃ¨le nomic-embed-text :
  ollama pull nomic-embed-text
  ollama serve

Modifications Ã  faire :

1. Remplace l'import :
   - Commente : from sentence_transformers import SentenceTransformer
   - Ajoute : import ollama

2. Change le modÃ¨le :
   EMBEDDING_MODEL = "nomic-embed-text"

3. CrÃ©e une nouvelle fonction get_embedding(text) :
   - response = ollama.embeddings(model=EMBEDDING_MODEL, prompt=text)
   - return response["embedding"]
   - Ajoute gestion d'erreur avec message clair

4. Supprime ou commente get_embedding_model()

5. Modifie create_embeddings(chunks) :
   - Boucle sur chaque chunk
   - chunk["embedding"] = get_embedding(chunk["text"])

6. Modifie search_similar(query) :
   - query_embedding = get_embedding(query)
   - Supprime l'appel Ã  get_embedding_model()

Garde l'ancienne version en commentaire pour rÃ©fÃ©rence.
```

**ğŸ’¡ IntÃ©rÃªt pÃ©dagogique** :

Cette migration montre comment optimiser progressivement :
- **Avant** (sentence-transformers) : tÃ©lÃ©charge ~500 Mo, plus lent
- **AprÃ¨s** (Ollama) : instantanÃ©, local, mÃªme modÃ¨le rÃ©utilisable

**Avantages Ollama** :
- âš¡ **Performance** : Embeddings instantanÃ©s
- ğŸ”’ **SouverainetÃ©** : 100% local, aucun appel externe
- ğŸ¯ **SimplicitÃ©** : Un seul modÃ¨le partagÃ© pour tous les projets
- ğŸ’¾ **Ã‰conomie** : Pas de duplication des modÃ¨les Python

**DÃ©monstration suggÃ©rÃ©e** :
1. Montrer la vitesse avant/aprÃ¨s avec un document de test
2. Expliquer que c'est la mÃªme qualitÃ© d'embeddings
3. Montrer `ollama list` pour voir les modÃ¨les disponibles

---

## ğŸ¯ Workflow de la dÃ©mo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. git checkout eb97b5f     â† Point de dÃ©part              â”‚
â”‚  2. claude                   â† Lancer Claude Code           â”‚
â”‚  3. [Coller le prompt]       â† Expliquer puis exÃ©cuter      â”‚
â”‚  4. streamlit run app.py     â† Tester le rÃ©sultat           â”‚
â”‚  5. [Expliquer le code]      â† Montrer ce qui a Ã©tÃ© gÃ©nÃ©rÃ©  â”‚
â”‚  6. Passer Ã  l'Ã©tape suivante...                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš¡ Si le code gÃ©nÃ©rÃ© a un bug

```bash
# Option 1 : Demander une correction
claude "J'ai cette erreur : [coller l'erreur]. Corrige le code."

# Option 2 : Utiliser le commit backup
git checkout -- app.py
git checkout <hash_backup>
```

---

## ğŸ’¡ Commandes Claude Code utiles

| Action | Commande |
|--------|----------|
| Corriger une erreur | `claude "Erreur: [message]. Corrige."` |
| Expliquer le code | `claude "Explique la fonction search_similar"` |
| Voir les fichiers modifiÃ©s | `git status` |
| Voir le diff | `git diff app.py` |
| Annuler tout | `git checkout -- .` |
| Committer | `git add -A && git commit -m "message"` |

---

## ğŸ“‹ Phrases de transition (Ã  dire pendant la dÃ©mo)

| Ã‰tape | Phrase |
|-------|--------|
| 1â†’2 | "Demandons Ã  Claude de crÃ©er l'interface..." |
| 2â†’3 | "L'interface est prÃªte, connectons-nous Ã  Aristote..." |
| 3â†’4 | "On est connectÃ© ! Faisons fonctionner le chat..." |
| 4â†’5 | "Le chatbot marche ! Passons au RAG. PremiÃ¨re Ã©tape : l'upload..." |
| 5â†’6 | "On peut charger des fichiers, extrayons le texte..." |
| 6â†’7 | "Le texte est extrait, dÃ©coupons-le en chunks..." |
| 7â†’8 | "Les chunks sont prÃªts, vectorisons-les avec Ollama..." |
| 8â†’9 | "On a les embeddings, stockons-les dans ChromaDB..." |
| 9â†’10 | "La base vectorielle est prÃªte, connectons le RAG au chat..." |
| 10â†’11 | "Le RAG fonctionne ! Ajoutons les finitions..." |
| 11â†’12 | "Maintenant, sÃ©curisons les rÃ©ponses avec le mode exclusif..." |
| 12â†’14 | "Pour finir, optimisons les performances avec Ollama..." |

---

## ğŸ“Š RÃ©fÃ©rence des commits backup

| # | Hash | Ã‰tape | FonctionnalitÃ© |
|---|------|-------|----------------|
| 1 | `eb97b5f` | Initial | Structure projet |
| 2 | `189e823` | Interface | Streamlit de base |
| 3 | `393a206` | Connexion | API Aristote |
| 4 | `a9ad6b4` | Chat | Conversation fonctionnelle |
| 5 | `bf9e480` | Upload | Chargement fichiers |
| 6 | `3bfbdf4` | Extraction | PDF/DOCX â†’ texte |
| 7 | `ba88d3a` | Chunking | DÃ©coupage en morceaux |
| 8 | `0faee37` | Embeddings | Vectorisation *(sentence-transformers)* |
| 9 | `bd894e1` | ChromaDB | Base vectorielle |
| 10 | `d33aafe` | RAG | Recherche + injection |
| 11 | `f55f9bd` | Polish | Finitions UX |
| 12 | `06e5a4f` | **RAG exclusif** | Anti-hallucination |
| 13 | `dca52ac` | Fix | CompatibilitÃ© NumPy < 2.0 |
| 14 | `2b6bf6f` | **Ollama** | Migration embeddings optimisÃ©s ğŸ¦™ |
| 15 | `07281ce` | **Fix chunking** | Correction boucle infinie âš ï¸ |

---

## âš ï¸ Bugs connus et correctifs

### Bug 1 : MemoryError dans chunking (Commit 15)

**SymptÃ´me** : `MemoryError` lors du traitement d'un document, l'app freeze

**Cause** : Boucle infinie dans `chunk_text()` si `overlap >= taille rÃ©elle du chunk`

**Solution** : Appliquer le commit `07281ce` ou utiliser le code corrigÃ© :
```python
# Ã€ la fin de la boucle while dans chunk_text()
next_start = end - overlap
if next_start <= start:
    start = start + 1  # Garantir progression
else:
    start = next_start
```

**Quand l'appliquer** : DÃ¨s l'Ã©tape 7 si vous rencontrez le problÃ¨me, ou Ã  la fin de la dÃ©mo

---

## ğŸ¦™ Progression embeddings : sentence-transformers â†’ Ollama

La dÃ©mo montre une **migration progressive** :
- **Commit 8** : embeddings avec `sentence-transformers` (fonctionne partout)
- **Commit 14** : migration vers `Ollama` (optimisation locale)

| Aspect | sentence-transformers (Commit 8) | Ollama (Commit 14) |
|--------|----------------------------------|---------------------|
| Installation | TÃ©lÃ©charge ~500 Mo | DÃ©jÃ  sur ta machine |
| Vitesse premier run | Lent (tÃ©lÃ©chargement) | InstantanÃ© |
| Performance runtime | Bon | Excellent |
| ModÃ¨le | paraphrase-multilingual-MiniLM | nomic-embed-text |
| SouverainetÃ© | Local (aprÃ¨s download) | 100% local |
| RÃ©utilisation | 1 modÃ¨le par projet Python | 1 modÃ¨le partagÃ© |

**IntÃ©rÃªt pÃ©dagogique** : Montrer qu'on peut optimiser progressivement sans tout refaire !

---

## âœ… Checklist avant la dÃ©mo

- [ ] Ollama lancÃ© (`ollama serve` ou app desktop)
- [ ] ModÃ¨le installÃ© (`ollama pull nomic-embed-text`)
- [ ] VÃ©rification : `curl http://localhost:11434/api/tags`
- [ ] Token Aristote prÃªt dans `.env`
- [ ] Documents de test (2-3 PDF/DOCX courts)
- [ ] Projet clonÃ© et au commit initial (`git checkout eb97b5f`)
- [ ] Claude Code installÃ© (`npm install -g @anthropic-ai/claude-code`)
- [ ] Ce fichier accessible sur un 2Ã¨me Ã©cran
